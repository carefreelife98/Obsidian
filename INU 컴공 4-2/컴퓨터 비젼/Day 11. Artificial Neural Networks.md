---
Author: CarefreeLife98
Date: 2023-11-17T14:36:00
Agenda: 
tags:
  - Computer_Vision
---
# Artificial neural networks (ANNs)
![[ìŠ¤í¬ë¦°ìƒ· 2023-11-17 ì˜¤í›„ 2.58.29.png]]
> **Multilayer perceptron : misnomer**
> - network of multiple logistic models (continous nonlinearity)
> - Perceptronì€ ë¶ˆì—°ì†ì„±ì´ì§€ë§Œ misnomer ì€ ì—°ì†ì ì¸ íŠ¹ì„±ì„ ê°€ì§.
> 
> <br>
> ë©”ì¸ ì•„ì´ë””ì–´
> - ì¶”ì¶œëœ input ì— ëŒ€í•œ ì„ í˜• ì¡°í•©ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œ.
> - ê·¸ê²ƒì— ëŒ€í•˜ì—¬ Activation Function ì„ ì·¨í•¨ (?)

# Models of a Neuron
> 1. **Synapses**
> 	- weights ë¡œì„œ êµ¬í˜„
> 2. **Adder**
> 	- sumation.
> 	- input vector -> scalar
> 3. **Activation function (Possibly Nonlinear)**


## Activation function
![[ìŠ¤í¬ë¦°ìƒ· 2023-11-17 ì˜¤í›„ 3.26.14.png]]

# ReLU (ì¤‘ìš”)
> **ê°€ì¥ ë§ì´ ì“°ì´ëŠ” Linear Unit.**
> - **ì¥ì **
> 	- **back-propagation ìˆ˜í–‰ ì„±ëŠ¥ ì¢‹ìŒ.**
> 	- ì¶”í›„ **Vanishing gradient ë¬¸ì œ í•´ê²°** ê°€ëŠ¥
> 		- Vanishing gradient: Layer ê°€ ì ì  ê¹Šì–´ì§€ë©´ì„œ gradient ê°€ íë¦¿í•´ì§.
> - **í•œê³„**
> 	- í”„ë¡œê·¸ë¨ì´ ì£½ì„ ìˆ˜ ìˆìŒ.
> 		- Leaky ReLU ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¹ë³µ ê°€ëŠ¥.

<br><br>
# Multilayer Perceptron
## Single-layer feedforward network
![[ìŠ¤í¬ë¦°ìƒ· 2023-11-17 ì˜¤í›„ 4.08.28.png]]
> **íŒŒë€ìƒ‰ ë°•ìŠ¤ : Input** <br>
> **íšŒìƒ‰ ì› : neuron (= perceptron)**
> - **ë‘˜ ì‚¬ì´ì— ì—°ê²°ì´ ì¡´ì¬í•œë‹¤ == ë‘˜ ì‚¬ì´ì— weight ê°€ ì¡´ì¬í•œë‹¤.**
> <br>
> **ê° ì…ë ¥ê³¼ Perceptron ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜(weight) ë¥¼ Matrix í˜•íƒœë¡œì„œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ.**
> - Input layer (ì…ë ¥ Layer)
> - Output layer (Perceptron Layer)
> <br>
> **Forwarded**
> - ëª¨ë“  Signal ì€ì…ë ¥ì—ì„œ Output(Perceptron) ë°©í–¥ìœ¼ë¡œë§Œ ì§„í–‰.
> - ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œëŠ” ë¶ˆê°€.

<br><br>
## Multilayer feedforward network (Multilayer Perceptron)
![[ìŠ¤í¬ë¦°ìƒ· 2023-11-17 ì˜¤í›„ 4.23.27.png]]
> **íƒ„ìƒ ë°°ê²½**
> - XNOR ê³¼ ê°™ì€ ë¶„í¬ëŠ” ì„ í˜• ëª¨ë¸ë¡œì„œ ì •í™•í•œ í´ë˜ìŠ¤ ë¶„ë¥˜ê°€ ë¶ˆê°€ëŠ¥ í–ˆìŒ.
> - ì´ì— **Layer (Perceptron) ë¥¼ ì—¬ëŸ¬ ê°œ ìŒ“ì•„ ì˜¬ë¦¼ìœ¼ë¡œì„œ Multilayer ë¥¼ ë§Œë“¤ì–´ Nonlinear ë¬¸ì œë¥¼ í•´ê²°.**

![[ìŠ¤í¬ë¦°ìƒ· 2023-11-17 ì˜¤í›„ 4.25.59.png]]
> **Input layer ì™€ Output layer ì‚¬ì´ì— ì—¬ëŸ¬ ê°œì˜ hidden layer ê°€ ì¡´ì¬í•¨.**
> - **ê° Layer ì˜ Output ì´ ë‹¤ìŒ Layer ì˜ Input ì´ ë˜ì–´ Foward ë°©í–¥ìœ¼ë¡œ ì§„í–‰.**

<br><br>

## Types of signals in neural networks
![[ìŠ¤í¬ë¦°ìƒ· 2023-11-17 ì˜¤í›„ 4.34.20.png]]
### 1. Function Signal
> ì´ì „ì— ë°°ìš´ê²ƒ.
> - **Input ìœ¼ë¡œ Input signal ì´ ë“¤ì–´ì™€ Output ìœ¼ë¡œ Output signal ì´ ë‚˜ê°€ëŠ” ê²ƒ. (Forward)**

### 2. Error Signal
> **ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê°’ê³¼ ì‹¤ì œ ê²°ê³¼ ê°’ì„ ë¹„êµí•˜ì—¬ Error signal ì„ ì°¾ì•„ë‚¼ ìˆ˜ ìˆìŒ.**
> - **Back propagation**


<br><br>

## Softmax Function
![[ìŠ¤í¬ë¦°ìƒ· 2023-11-17 ì˜¤í›„ 4.35.07.png]]
> **10ê°œ í´ë˜ìŠ¤ê°€ ë„ì¶œë˜ì–´ 10ì°¨ì›ì˜ Vector ë¡œì„œ ë°ì´í„°ê°€ ìƒì„±ëœ ê²½ìš°, ê° ë°ì´í„°ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ í™•ë¥  ë¶„í¬ë¡œì„œ í‘œí˜„í•´ì£¼ëŠ” Function.**
> - **ê° Data ì— ì‹œê·¸ë§ˆ í•¨ìˆ˜ë¥¼ ì”Œì›Œ í™•ë¥  ë¶„í¬ë¡œì„œ í‘œí˜„.**
> 	- ì´ ê²½ìš° ëª¨ë“  Data ë¥¼ ë”í•˜ë©´ 1ì´ ëœë‹¤. (í™•ë¥  ë¶„í¬ë¡œì„œ í‘œí˜„ëœ ë°ì´í„° ì´ë¯€ë¡œ)

<br><br>
### Softmax function ì˜ˆì‹œ
![[ìŠ¤í¬ë¦°ìƒ· 2023-11-17 ì˜¤í›„ 4.40.33.png]]
> **ê¸°ì¡´ Data ì— Softmax ë¥¼ ì”Œìš°ë”ë¼ë„ Data ê°„ì˜ í¬ê¸° ìˆœì„œëŠ” ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤.**
> - ê° Data ì˜ ê°’ì„ 0 ~ 1 ì‚¬ì´ì˜ í™•ë¥ ì  ê°’ìœ¼ë¡œ ë³€í™˜ ì‹œì¼œì£¼ëŠ” ê²ƒ.
> <br>
> **Softmax Function í›„ ë„ì¶œëœ Data ì— ëŒ€í•œ ì¼ë°˜ì ì¸ í•´ì„**
> - íŠ¹ì • Class ì˜ í™•ë¥  ê°’ì´ ë†’ë‹¤ë©´, Input Data(ì´ë¯¸ì§€)ëŠ” í•´ë‹¹ Class ì¼ í™•ë¥ ì´ ë†’ë‹¤.

<br><br>

## Function of Hidden Neurons
![[ìŠ¤í¬ë¦°ìƒ· 2023-11-17 ì˜¤í›„ 4.45.33.png]]
> **ì´ë ‡ê²Œ ì¶”ì¶œí•œ Data ë“¤ì€ ê·¸ ìì²´ë¡œì„œ Data ë¥¼ í‘œí˜„í•˜ì§€ëŠ” ëª»í•˜ì§€ë§Œ í•´ë‹¹ Data ë“¤ì€ ì‹¤ì œ Data ë“¤ì´ ë…¹ì•„ë“¤ë©° ë§Œë“¤ì–´ì§„ ê²ƒì´ë¯€ë¡œ í•´ë‹¹ Data ë“¤ì˜ êµ°ì§‘ì„ í†µí•´ Image(Data) ì˜ íŠ¹ì§•ì„ ë½‘ì•„ ë‚¼ ìˆ˜ ìˆë‹¤.**

<br><br>

# Multilayer Perceptron
![[ìŠ¤í¬ë¦°ìƒ· 2023-11-18 ì˜¤í›„ 5.53.29.png]]
> **Neural network ì—ì„œ Signal ì˜ Type**
> 1. **Function Signal**
> 	- input ìœ¼ë¡œ input signal ì´ ë“¤ì–´ì™€ network ë¥¼ íƒ€ê³  ì „ë°©ìœ¼ë¡œ ì „íŒŒë˜ì–´ ë‚˜ê°€ë©° output ìœ¼ë¡œ ë°˜í™˜.
> 	<br><br>
> 2. **Error Signal : Back propagation**
> 	- output neuron ìœ¼ë¡œë¶€í„° ì´ì „ Layer ë¡œ ì´ë™í•˜ë©° ì´ˆê¸°ê°’ê³¼ì˜ ë¹„êµë¥¼ í†µí•´ Error ë¥¼ ê²€ì¶œ

<br><br>
## Training by Forward / Backward Propagation
> ![[ìŠ¤í¬ë¦°ìƒ· 2023-11-18 ì˜¤í›„ 6.12.09.png]]
> **Training ì´ë€ ì–´ë–¤ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ëŠ” ê³¼ì •.**
> - ìµœì ì˜ ê°€ì¤‘ì¹˜ë¥¼ íƒìƒ‰í•˜ëŠ” ê³¼ì •.
> <br>
> **Training ì˜ ê³¼ì •**
> 1. **ì£¼ì–´ì§„ Dataë¥¼ í•œë²ˆ í˜ë ¤ì¤€ë‹¤.**
> 	- Model ì˜ **Forward propagation** ì„ ìˆ˜í–‰.
> 	- Forward propagation ì— ë”°ë¥¸ Output ë°œìƒ.
> 2. **Backward propagation ìˆ˜í–‰**
> 	- ì‹¤ì œ Data ì˜ Label ê³¼ Output ê°„ì˜ **ì°¨ì´, ì°¨ì´ì˜ ì œê³±ê°’, ì ˆëŒ€ê°’ ë“±ìœ¼ë¡œ Error ë¥¼ ì •ì˜.**
> 	- **Error ì— ë”°ë¼ w(weight) ë¥¼ ê°±ì‹ .**
> 3. **1, 2 ì™€ ê°™ì€ Forward, backward propagation ì˜ ë°˜ë³µì„ í†µí•´ Model ì˜ Parameter ë¥¼ ê³„ì† ê°±ì‹ **í•´ë‚˜ê°. (Iterator)
> 4. **Training ì´ ì™„ë£Œë˜ë©´ Model ì— ìµœì í™”ëœ w(weight) ê°’ì´ ì„¤ì •ëœë‹¤.**
> 	- w ê°’ì€ freeze ë˜ì–´ ìˆìŒ.
> <br><br>
> **Test ë€ ê°€ì¤‘ì¹˜ë¥¼ ê³ ì •í•œ ìƒíƒœì—ì„œ ìš°ë¦¬ê°€ ê°€ì§€ê³  ìˆëŠ” Test Data ì— ëŒ€í•œ í‰ê°€ë¥¼ í•˜ëŠ” ê²ƒ.**
> - ìœ„ì˜ Training ê³¼ì •ì—ì„œ ìµœì í™”ëœ ê°€ì¤‘ì¹˜ ê°’ì„ ê°€ì§€ê³  ìˆëŠ” Modelì— ìƒˆë¡œìš´ Test Data(image) ë¥¼ Inputìœ¼ë¡œ ì…ë ¥í•˜ì—¬ Testë¥¼ ìˆ˜í–‰.

<br><br>

## Assumptions and Notations
![[ìŠ¤í¬ë¦°ìƒ· 2023-11-18 ì˜¤í›„ 6.53.36.png]]
> **d-H-K neutral network (MLP: Multilayer Pereptron)**
> - **d: ì…ë ¥ì˜ ì°¨ì›**
> 	- ì£¼ì–´ì§„ Data ì—ì„œ ì¶”ì¶œí•´ë‚¸ íŠ¹ì§•ì˜ ê°œìˆ˜ë§Œí¼ ì…ë ¥ì˜ ì°¨ì›ì´ ì¡´ì¬.
> - **H: input ê³¼ output ì‚¬ì´ì˜ hidden layer ì˜ ê°œìˆ˜**
> - **K: Output ì˜ ê°œìˆ˜ (ì°¨ì›).**
> 	- Output ì€ Class ì˜ ê°œìˆ˜ë§Œí¼ ë°˜í™˜.
> 	- Data ë¥¼ **ëª‡ ê°œì˜ Class ë¡œ êµ¬ë¶„í•  ê²ƒì¸ì§€**ë¥¼ ë‚˜íƒ€ëƒ„.
> 
> <br><br>
> **ê°€ì¤‘ì¹˜ í‘œí˜„ ë°©ì‹**
> - input x_i ì™€ hidden layer ì˜ neuron_j ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•œë‹¤.
> 	- **`w_ji (Backward ë°©í–¥ìœ¼ë¡œ index ë¥¼ í‘œê¸°)`**
> 
> <br><br>
> **Model ì˜ ê°€ì¤‘ì¹˜ ì •ì˜**
> - **(x -> z) ì˜ ê°€ì¤‘ì¹˜ = w_1**
> 	- **d x H Matrix** ë¡œì„œ í‘œí˜„ ê°€ëŠ¥
> - **(z -> h) ì˜ ê°€ì¤‘ì¹˜ = w_2**
> 	- **H x K Matrix** ë¡œì„œ í‘œí˜„ ê°€ëŠ¥.
> - ë”°ë¼ì„œ, **Model ì˜ ì´ Parameter (w)** ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.
> 	- **H = {w_1, W_2}**

<br><br>

## Error measures
![[ìŠ¤í¬ë¦°ìƒ· 2023-11-18 ì˜¤í›„ 7.15.26.png]]
> **ì„¸ ê°€ì§€ Error measure ì´ ì¡´ì¬.**
> <br>
> 
> 1. **e_k**
> 	- **e_k : k ë²ˆì§¸ Output ì— ëŒ€í•œ Error signal.**
> 		- Output_k ì™€ ì‹¤ì œ Data ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ëƒ„.
> 2. **E_n**
> 	- **E_n : ë§ì€ Data ì¤‘ n ë²ˆì§¸ Data ì— ëŒ€í•œ Error signal ì˜ ì´í•©.**
> 		- í•œ Data ì— ëŒ€í•œ ëª¨ë“  e_k ì˜ ì´í•©.
> 		- n ì˜ í¬ê¸°ì— ì˜í–¥ì„ ë°›ìŒ.
> 3. **E_D**
> 	- **E_D : ëª¨ë“  Data ì˜ Error signal ì´í•©.**
> 		- ëª¨ë“  Data ì— ëŒ€í•œ E_n ì˜ ì´í•©.
> 		- ì „ì²´ì— ëŒ€í•œ Error ë¥¼ ë‚˜íƒ€ëƒ„.

<br><br>

# Back Propagation
> **ê°€ì¥ ì´ìƒì ì¸ ê°€ì¤‘ì¹˜ w ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜.**

## Credit Assignment Problem
> Output layer ì—ëŠ” ì •ë‹µ y_k (Target) ê°€ ì¡´ì¬.
> - e_k = h_k - y_k ë¡œì„œ ë¹„êµì  ì‰½ê²Œ ì—ëŸ¬ ì •ì˜ ê°€ëŠ¥.
> 
> <br>
> í•˜ì§€ë§Œ, Input ê³¼ Hidden layer ì‚¬ì´ì˜ ì—ëŸ¬ëŠ” ì •ì˜ê°€ ì–´ë ¤ì›€
> - **ëª…í™•í•œ ì •ë‹µì´ Hidden layer ì— ì¡´ì¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸.**
> - **ë”°ë¼ì„œ, Back propagation ì‚¬ìš©í•˜ì—¬ ì—ëŸ¬ì˜ ì •ì˜ê°€ ê°€ëŠ¥í•¨.**

<br><br>

## Sensitivity Factor and Delta Error (Output layer)
> **Delta Error : K ë²ˆì§¸ Output ì— ëŒ€í•œ Error.**
> <br>
> **Sensitivity Factor ë¥¼ í†µí•´ ê° w(ê°€ì¤‘ì¹˜) ì˜ ì¡°ì • ì •ë„ë¥¼ êµ¬í•œë‹¤.**
> - Sensitivity Factor ë° Delta Error ê³„ì‚° ì‹œ Chain rule ì ìš©.
> 	- **Temp ë³€ìˆ˜ì¸ round_sk ë¥¼ ì‚¬ìš©í•˜ì—¬ í¸ë¯¸ë¶„.**
> 
> <br>
> **Sensitivity Factor**
> - ğ›¿_ğ‘˜ â‹… ğ‘§_ğ‘—
> 
> **Delta Error**
> - ğ‘’_ğ‘˜ â‹… ğœ'(ğ‘ _ğ‘˜)
> <br>
> **Weight update rule**
> - Sensitivity Factor ì— ì—íƒ€? (âˆ’ğœ‚ : learning rate) ë¥¼ ê³±í•´ì„œ ì •ì˜
> 	- **ì—íƒ€(learning rate): ê°€ì¤‘ì¹˜ì˜ ì¡°ì • ì •ë„ê°€ ë„ˆë¬´ ê¸‰ê²©í•˜ì§€ ì•Šê³  ì ë‹¹í•˜ë„ë¡ Scaling.**
> - **Î”ğ‘¤_ğ‘˜ğ‘— = âˆ’ğœ‚ â‹… ğ›¿_ğ‘˜â‹… ğ‘§_ğ‘—**

```python
1: initialize all weights {ğ‘¤_ğ‘—ğ‘–, ğ‘¤_ğ‘˜ğ‘–}
2: repeat
3:     pick ğ‘› âˆˆ {1, 2, â€¦ , ğ‘}
4:     forward: compute all
5:     backward: compute all
6:     update weights:
			hidden(ğ‘—)-to-output(ğ‘˜): ğ‘¤_ğ‘˜ğ‘— â† ğ‘¤_ğ‘˜ğ‘— âˆ’ ğœ‚ â‹… ğ›¿_ğ‘˜â‹… ğ‘§_ğ‘—
			input(ğ‘–)-to-hidden (ğ‘—): ğ‘¤_ğ‘—ğ‘– â† ğ‘¤_ğ‘—ğ‘– âˆ’ ğœ‚ â‹… ğ›¿_ğ‘— â‹… ğ‘¥_ğ‘–
7: until it is time to stop
8: return final weights {ğ‘¤_ğ‘—ğ‘– , ğ‘¤_ğ‘˜ğ‘—}
```