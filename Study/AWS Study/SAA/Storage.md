  

- 단일 Amazon S3 버킷에 데이터를 최대한 빨리 집계 하는 법  
    [고속 인터넷 연결, 글로벌 사이트 데이터 500GB, 운영 복잡성 최소화]
    - 대상 S3 Bucket에서 `S3 Transfer Acceleration` 을 키고 `멀티 파트 업로드`를 사용하여 사이트 데이터를 대상 S3 버킷에 직접 업로드.

  

- S3 버킷에 JSON 형식으로 저장된 독점 애플리케이션 로그 파일을 쿼리를 통해 분석  
    [최소한의 변경 및 운영 오버헤드]
    - `S3에 쿼리하는 건 AWS Athena`
    - S3와 함께 Amazon Athena를 직접 사용하여 필요에 따라 쿼리를 실행

  

- [Q5]  
    사용자가 업로드한 문서를 EBS 볼륨에 저장하는 EC2 인스턴스 운용 중.  
    확장성 및 고 가용성을 위해 다른 AZ에 두번째 EC2 , EBS Volume 생성 및 ALB 뒤에 배치.  
    사용자가 웹 사이트 새로고침 시 모든 문서를 보지 못하고 일부 문서만 접근됨.  
    사용자가 모든 문서를 한번에 볼 수 있도록 제안 사항은?
    - 두 EBS 볼륨의 기존 데이터를 Amazon EFS 로 복사.  
        이후 새 문서를 Amazon EFS 에 저장하도록 애플리케이션을 수정.
    - EBS 와 EFS 의 큰 차이점
        - `EBS 는 단일 AZ 내부`에서만 접근 가능.  
            `EFS 는 다중 AZ` 에서 접근 가능.
        - 각 AZ 별로 별도의 EBS 볼륨을 사용함에 AZ 간 공유가 되지 않아 문제 발생한 것.
        - Application 서버가 다중 AZ에서 접근 가능한 EFS에 데이터를 저장하도록 수정.

  

- [Q6]  
    NFS 를 통해 On-Premise 환경에 저장하던 1MB ~ 500GB 의 대용량 비디오 파일을 S3 로 Migration.  
    [최소한의 네트워크 대역 사용, 최대한 빠르게 Migration]
    - 가능한 한 최소한의 네트워크 대역폭을 사용하라 했으니 아예 오프라인에서 Snowball Edge 로 올리는 게 맞음.
    - AWS Snowball 및 AWS Snowball Edge 는 기존 저장소에서 네트워크 대역폭이 충분하지 않을 때, 대용량 데이터 세트를 클라우드로 이전하는데 도움.
    - `Snowball 장치는 80TB` , `Snowball Edge 는 100TB` 까지 한번에 이동 가능.

  

- [Q9]  
    IDC 에서 SMB 파일 서버 실행 중.  
    파일서버는 파일 생성 후 자주 Access 하는 대용량 파일을 저장.  
    7일이 지나면 파일에 거의 Access 하지 않는다.  
    최근 Access 파일에 대한 저지연 Access 유지하는 동시에 저장공간 확장 및 파일 수명 주기 관리 제공하는 법?
    - Amazon S3 파일 게이트웨이를 생성하여 회사의 스토리지 공간을 확장.
    - S3 수명 주기 정책을 생성하여 7 일 후에 데이터를 `S3 Glacier Deep Archive` 로 전환
        
        - `사용 가능한 스토리지 공간의 확장 = Storage Gateway`
        - `Storage Gateway` 는 On-premise Strage 와 AWS Storage 를 합쳐 `사실상 무제한의 Storage`를 향유하는 것을 목적으로 하는 서비스.
        
          
        

  

- [Q20]  
    동일한 AWS 리전의 테스트 환경에 대량의 프로덕션 데이터를 복제하는 기능을 개선.  
    데이터는 Amazon Elastic Block Store(Amazon EBS) 볼륨의 Amazon EC2 인스턴스에 저장.  
    복제된 데이터를 수정해도 프로덕션 환경에 영향을 주지 않아야 함.  
    프로덕션 데이터를 테스트 환경에 복제하는 데 필요한 시간을 최소화.
    1. 프로덕션 EBS 볼륨의 EBS 스냅샷 생성.
    2. EBS 스냅샷에서 `EBS 빠른 스냅샷 복원` 기능 On.
    3. 스냅샷을 새 EBS 볼륨으로 복원.
    4. 테스트 환경의 EC2 에 새 EBS 볼륨을 연결.
        - [참고]
            - `인스턴스 스토어 볼륨은 휘발성`이라 꺼지면 데이터 삭제.
            - `EBS 다중 연결`을 사용하게 되면 `복제된 데이터를 수정할 때 프로덕션 환경에 영향`
            - `스냅샷은 새로운 볼륨을 만드는 것.`
                - `만들어진 볼륨에 스냅샷을 복원하는 것이 아님.`

  

- [Q22]  
    Amazon S3 를 사용하여 새로운 디지털 미디어 애플리케이션의 스토리지 아키텍처를 설계.  
    미디어 파일은 가용 영역 손실에 대한 복원력이 있어야 함.  
    일부 파일은 자주 액세스되는 반면 다른 파일은 예측할 수 없는 패턴으로 거의 액세스 되지 않음.  
    미디어 파일을 저장하고 검색하는 비용을 최소화.  
    스토리지 옵션은?  
    A. S3 Standard (S3 표준)  
    B. S3 Intelligent-Tiering (S3 지능형 계층화)  
    C. S3 Standard-Infrequent Access(S3 Standard-IA)  
    D. S3 One Zone-Infrequent Access(S3 One Zone-IA)
    - B. S3 Intelligent-Tiering (S3 지능형 계층화)
        - `S3 Intelligent-Tiering`  
            `액세스 빈도 또는 불규칙한 사용 패턴을 모를 때` 완벽한 사용 사례.
        - `예측할 수 없는 패턴 = S3 Intelligent Tiering`

  

- [Q23]  
    Amazon S3 Standard 스토리지를 사용하여 백업 파일을 저장.  
    1 개월 동안 파일에 자주 액세스. 단, 1 개월 이후에는 파일에 접근하지 않는다.  
    파일은 무기한 보관.  
    가장 비용 효율적으로 충족하는 스토리지 솔루션은?
    - S3 수명 주기 구성을 생성.  
        1 개월 후에 S3 Standard 에서 `S3 Glacier Deep Archive` 로 객체를 전환
    - `Amazon S3 Glacier Deep Archive`
        - `거의 액세스하지 않고 몇 시간의 검색 시간이 허용되는 데이터의 장기 보존`을 위한 안전하고 내구성이 있으며 `매우 저렴`한 Amazon S3 스토리지 클래스
    - `1 개월 이후 파일에 접근하지 않음 = S3 Glacier Deep Archive.`

  

- [Q39] `IOPS SSD`  
    웹 사이트에서 검색 가능한 항목 저장소를 유지 관리.  
    데이터는 천만 개 이상의 행이 포함된 Amazon RDS for MySQL 데이터베이스 테이블에 저장.  
    데이터베이스에는 2TB 의 범용 SSD 스토리지가 존재.  
    회사 웹 사이트를 통해 이 데이터에 대한 수백만 건의 업데이트가 매일 있음.  
    이 회사는 일부 삽입 작업이 10 초 이상 걸리는 것을 확인.  
    회사는 데이터베이스 스토리지 성능이 문제라고 판단.  
    이 성능 문제를 해결하는 솔루션은?
    
    - `스토리지 유형을 프로비저닝된 IOPS SSD 로 변경.`
        - '삽입' 작업이라고 했으므로 I/O 성능과 관련되어있음을 유추
        - '저장소 성능'이 문제라고 판단했고, 범용 'SSD' 스토리지가 있다고 했으므로 A 가 정답.
    - 프로비저닝된 IOPS 볼륨은 솔리드 스테이트 드라이브(SSD)로 지원되며 중요한 I/O 집약적인 데이터베이스 애플리케이션을 위해 설계된 최고 성능의 EBS 볼륨
    
      
    

  

- [Q44] `S3 - 삭제로부터 데이터 보호   `회사에 중요한 데이터가 포함된 Amazon S3 버킷이 있습니다. 회사는 우발적인 삭제로부터 데이터를 보호해야 한다.  
    요구 사항을 충족하기 위해 솔루션 설계자는 어떤 단계 조합을 취해야 합니까?  
    A. S3 버킷에서 버전 관리를 활성화합니다.  
    B. S3 버킷에서 MFA 삭제를 활성화합니다.  
    C. S3 버킷에 버킷 정책을 생성합니다.  
    D. S3 버킷에서 기본 암호화를 활성화합니다.  
    E. S3 버킷의 객체에 대한 수명 주기 정책을 생성합니다.
    - [A, B]
        - A : 버전 관리는 실수로 삭제했을 때 이전 버전의 파일을 불러올 수 있도록 해줌.
        - B : MFA Delete 는 함부로 삭제하지 못하도록 막음.
            - MFA Delete 는 다음 작업에 대해 추가 인증을 요구
                
                - 버킷의 버전 관리 상태 변경
                - 객체 버전 영구 삭제
                
                  
                

  

- [Q71]  
    한 회사는 Amazon DynamoDB 를 사용하여 고객 정보를 저장하는 쇼핑 애플리케이션을 실행.  
    데이터 손상의 경우 솔루션 설계자는 15 분의 RPO(복구 시점 목표)와 1 시간의  
    RTO(복구 시간 목표)를 충족하는 솔루션을 설계.  
    이러한 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 권장해야 합니까?
    - DynamoDB 지정 시간 복구를 구성합니다. RPO 복구의 경우 원하는 시점으로 복원합니다.
        - DynamoDB 는 주문형 백업 기능을 제공
        - 이를 통해 규정 준수 요구 사항에 대한 장기 보존 및 보관을 위해 테이블의 전체 백업을 생성할 수 있다.
        - 주문형 백업을 생성하고 Amazon DynamoDB 테이블에 대한 특정 시점 복구를 활성화할 수 있다.
        - 지정 시간 복구는 우발적인 쓰기 또는 삭제 작업으로부터 테이블을 보호하는 데 도움
        - 특정 시점 복구를 사용하면 지난 35 일 동안의 특정 시점으로 테이블을 복원할 수 있다.

  

- [Q78] `DynamoDB Backup`  
    회사는 사용자 트랜잭션 데이터를 Amazon DynamoDB 테이블에 보관해야 함.  
    회사는 데이터를 7 년간 보관해야 함.  
    이러한 요구 사항을 충족하는 가장 운영 효율성이 높은 솔루션은?
    - `AWS Backup` 을 사용하여 `테이블에 대한 백업 일정 및 보존 정책을 생성`
        - `한 곳에서 백업 현황 모니터링 및 콜드 스토리지에 저장, 예약 저장 가능`
        - `AWS Backup` 을 사용하면 `백업 정책을 구성하고 AWS 리소스 및 온프레미스 워크로드에 대한 활동을 한 곳에서 모니터링할 수 있음.`
        - `AWS Backup 과 함께 DynamoDB` 를 사용하면 AWS 계정 및 리전에서 `온디맨드 백업을 복사`하고, `온디맨드 백업에 비용 할당 태그를 추가`하고, `온디맨드 백업을 콜드 스토리지로 전환하여 비용 절감 가능`

  

- [Q85] `S3 Object Lock`  
    회사에 사용자가 웹 인터페이스 또는 모바일 앱을 통해 문서를 업로드하는 프로덕션 웹 애플리케이션 존재.  
    새로운 규제 요구 사항에 따라. 새 문서는 저장 후에 수정하거나 삭제할 수 없음.  
    솔루션 설계자는 이 요구 사항을 충족하기 위해 무엇을 해야 합니까?
    - 업로드된 문서를 `S3 버전 관리 및 S3 객체 잠금이 활성화된 Amazon S3 버킷에   저장.`
        - 수정하거나 삭제할 수 없음 = `S3 Object Lock.`
        - S3 객체 잠금을 사용하면 write-once-read-many(WORM) 모델을 사용하여 객체를 저장.
        - 객체 잠금은 고정된 시간 동안 또는 무기한으로 객체의 삭제 또는 덮어쓰기를 방지하는 데 도움.

  

- [Q93] `mysqldump`  
    회사는 MySQL 데이터베이스로 구동되는 온프레미스 애플리케이션을 실행.  
    회사는 애플리케이션의 탄력성과 가용성을 높이기 위해 애플리케이션을 AWS 로 마이그레이션 중.  
    현재 아키텍처는 정상 작동 시간 동안 데이터베이스에서 많은 읽기 활동.  
    회사의 개발 팀은 4 시간마다 프로덕션 데이터베이스의 전체 내보내기를 가져와 준비 환경의 데이터베이스에 삽입.  
    이 기간 동안 사용자는 허용할 수 없는 애플리케이션 대기 시간을 경험하고, 개발 팀은 절차가 완료될 때까지 스테이징 환경을 사용할 수 없음.  
    솔루션 설계자는 애플리케이션 지연 문제를 완화하는 대체 아키텍처를 권장해야 함.  
    대체 아키텍처는 개발 팀이 지연 없이 스테이징 환경을 계속 사용할 수 있는 능력을  
    제공해야 함.  
    어떤 솔루션이 이러한 요구 사항을 충족합니까?
    - **[Solution]**
        1. 프로덕션용 다중 AZ Aurora 복제본과 함께 Amazon Aurora MySQL 을 사용
        2. 데이터베이스 복제를 사용하여 요청 시 스테이징 데이터베이스를 생성.
    - RDS 에 비해 `Aurora 는 항상 3 개의 AZ 에 6 개의 복제본(Replica)를 보유`하고 있으므로 `애플리케이션 가용성에 더 유리`
    - `mysqldump는 뭔가요?`
        - `최대 1GB 정도의 소규모 데이터베이스라면 mysqldump 를 실행`
        - `mysqldump 는 데이터백업에 많은 시간이 소요되는 방법`